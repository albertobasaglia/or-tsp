\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{import}
\usepackage{subcaption}
\usepackage{mathtools}

\usepackage{fancyhdr}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\graphicspath{./assets/}

\title{Performance Analysis of TSP Solving Techniques}
\author{Basaglia Alberto, Stocco Andrea}
\date{\today}

\begin{document}
\pagestyle{fancy}

\maketitle

\begin{abstract}
The Traveling Salesman Problem (TSP) is definitely one of the most intensively studied problems in optimization.
It asks the following question:
\textit{Given a list of cities and the distances between each pair of cities, what is the shortest possible route
that visits each city exactly once and returns to the origin city?}
TSP is computationally difficult (NP-Hard), but there exist many \textit{heuristics} and \textit{exact methods} to solve it.
In this report we are going to show and compare the performances of these
techniques.
\end{abstract}

\section{Introduction}
The Traveling Salesman Problem is for sure one of the most intensively studied problems in
optimization. The TSP asks the question of what is the shortest path that visits exactly
once all the nodes in a graph.
We can also define it as finding the shortest Hamiltonian path of a graph.

The problem was formulated mathematically by the Irish mathematician William Rowan
Hamilton and by the British mathematician Thomas Kirkman in the 19th century~\cite{biggs1986graph}.

The state of the art for solving the TSP problem nowadays is definitely the Concorde
software. Concorde was created by  D. Applegate, R.E. Bixby, V. Chv√°tal, and W.J. Cook.
It employs advanced combinatorial optimization techniques, linear programming, and cutting-plane methods~\cite{applegate1998solution}\cite{tuHu2022analyzing}.

The objective of this report is not to invent some new techniques that can compete with Concorde,
but instead to cover basic approaches, that can be implemented without much effort, and still lead
to good results.

\section{Heuristics}
Heuristics are algorithms to determine a near-optimal solution to an optimization problem. In certain scenarios,
exact methods aren't able to provide the optimal solution in a reasonable amount of time or they can't find it
at all. In contrast, a heuristic is generally capable of offering a solution that is more or less close to
the optimal one.
While there is no assurance regarding the optimality of the provided solution, it may still be considered
acceptable in some instances.

\subsection{Greedy}
A greedy algorithm is a heuristic that attempts to find an optimal solution by selecting the locally best possible choice at each iteration.
For instance, in our case, when determining the next node to visit, the greedy algorithm will always choose
the nearest unvisited one. Algorithm~\ref{alg:greedy} shows the pseudocode of this procedure

\begin{algorithm}[ht]
\caption{Greedy}
\label{alg:greedy}
\hspace*{0.5em} \textbf{Output}: $solution$
\begin{algorithmic}
\Procedure{greedy}{$startingnode, nnodes$}
	\State $solution \gets {0, 1, \dots, nnodes - 1}$
	\State $solution \gets \Call{swap}{solution, 0, startingnode}$
        \For{$i \gets 0$ to $nnodes - 1$}
                \State $mindist \gets \infty$
                \State $minindex \gets -1$
                \For{$j \gets i + 1$ to $nnodes$}
                        \If{$dist(i, j) < mindist$}
                                \State $mindist \gets dist(i, j)$
                                \State $minindex \gets j$
                        \EndIf
                \EndFor
                \State $solution \gets \Call{swap}{solution, i + 1, minindex}$
        \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
If we run this approach on an instance having the cities on a 2d plane and using the
eucledian distance as the cost of the edges, we will se, with a very high probability, an
high number of edges crossing.
It is possible to prove that an optimal solution, in this context, should not contain any crossing
between edges.
This fact can give us the intuition that a very simple yet clever technique could be used
to drastically improve our solution. This technique, known as \textit{2-opt}, will be covered
in the next section.

Another important point to discuss is the node the algorithm is started on.
One of the strategies we could think of would be to generate one randomly.
Although feasible, in our experiment we decided to run the greedy procedure
on all the possible starting nodes and then keep the best.


\subsection{2-opt}
The 2-opt algorithm is an optimization technique that can be used to improve a sub-optimal solution.
It evaluates the possibility of removing two edges in the tour and reconnecting the now disconnected nodes in the other way.
This process aims to improve the cost of the tour by eliminating inefficient segments while preserving the overall tour structure. By
iteratively exploring these adjustments and accepting them if they result in a shorter tour cost, the algorithm gradually refines
the solution. This iterative refinement continues until no further improvements can be made, resulting in a
solution closer to the optimal one for the given problem instance.
Although it is not the only possible situation where a 2-opt move is
beneficial, figure~\ref{fig:2optmove} illustrates an example of such a move.

\begin{figure}[H]
        \caption{Example of a 2-opt move}
        \label{fig:2optmove}
        \centering
        \begin{subfigure}{.5\textwidth}
                \centering
                \def\svgwidth{.7\linewidth}
                \import{assets}{2opt-pre.pdf_tex}
                \caption{Before the move}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
                \centering
                \def\svgwidth{.7\linewidth}
                \import{assets}{2opt-post.pdf_tex}
                \caption{After the move}
        \end{subfigure}
\end{figure}

It's easy to see that the removal of the crossing edges is beneficial to the
cost of the solution.

Algorithm \ref{alg:greedy_2opt} shows a possible implementation of this procedure to improve
the solution found with the greedy approach. In this case, as discussed in the
previous section, the greedy procedure is executed for every possible node.
Then, for each of the solutions, the \textit{2-opt} procedure is applied.
Another approach, that would use \textit{2-opt} only once instead of $n$ times, would be
to compute greedy on all the starting nodes first, and then \textit{2-opt} only on the
best one. This approach won't be covered in this report.

\begin{algorithm}[ht]
\caption{Best 2-opt Swap}
\label{alg:best2optswap}
\hspace*{0.5em} \textbf{Output}: $bestsolution$
\begin{algorithmic}
\Procedure{best2optswap}{$solution, nnodes$}
\State $bestsolution \gets solution$
        \For{$i \gets 0$ to $nnodes - 2$}
                \For{$j \gets i + 2$ to $nnodes$}
                        \State $newsolution \gets \Call{reversesubsequence}{solution, i + 1, j}$
                        \If{$\Call{cost}{newsolution} < \Call{cost}{bestsolution}$}
                                \State $bestsolution \gets newsolution$
                        \EndIf
                \EndFor
        \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Greedy + 2-opt}
\label{alg:greedy_2opt}
\hspace*{0.5em} \textbf{Output}: $solution$
\begin{algorithmic}
\Procedure{greedy2opt}{$nnodes$}
        \State $solution \gets \emptyset$
        \State $solutioncost \gets \infty$
        \ForAll{$node \in tsp$}
                \State $currentsolution \gets \Call{greedy}{node, nnodes}$
                \While{$true$}
                        \State $2optsolution \gets \Call{best2optswap}{currentsolution, nnodes}$
                        \If{$\Call{cost}{2optsolution} < \Call{cost}{currentsolution}$}
                                \State $currentsolution \gets 2optsolution$
                        \Else
                                \State $break$
                        \EndIf
                \EndWhile
                \If{$\Call{cost}{currentsolution} < solutioncost$}
                        \State $solution \gets currentsolution$
                        \State $solutioncost \gets \Call{cost}{currentsolution}$
                \EndIf
        \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\clearpage

\section{Metaheuristics}
A metaheuristic is a versatile problem-solving approach characterized by its iterative nature and adaptability across various optimization problems. Unlike specific algorithms tailored to particular problems, metaheuristics serve as overarching strategies that guide subordinate heuristics to efficiently explore and exploit solution spaces. They intelligently combine different concepts to navigate through search spaces, aiming to find near-optimal solutions effectively.
In our experiments, we will adopt metaheuristic using \textit{2-opt} as the underlying
heuristic. We will prove that simple but very clever ideas (like Tabu Search and VNS) combined with the previously seen heuristics allow us to get solutions very close to the optimal ones.

\subsection{Tabu Search}
Tabu Search is a metaheuristic that efficiently explores the solution space by intelligently navigating through a neighborhood of solutions while maintaining a short-term memory to avoid revisiting previously visited or less promising solutions. The algorithm is particularly effective for combinatorial optimization problems like the Traveling Salesman Problem (TSP).

This technique was introducted by Fred W. Glover in 1986 and then formalized
in 1989~\cite{Glover:TabuSearch}.

By design, this procedure will initially head directly towards a local minimum, thanks to the underlying heuristic. The only purpose of this metaheuristic is then to escape it and, hopefully, converge to a better minimum.

The search procedure will be alternating between 2 different behaviors by its nature. First of all
we will identify a phase where the method, starting from a ``bad'' solution, uses the underlying
heuristic to improve its cost. We will call this ``intensification phase''. Once a local minimum is reached, we will apply some bad moves to escape from this solution. We name this the ``diversification phase''. We hope that the alternation between improving the solution and moving
away from it allows us to explore different local minimum, allowing us to get an overall better
solution.

Algorithm~\ref{alg:tabu} shows a very abstract description of the procedure we use in practice. The first solution is found using a greedy method (the closest neighbor apporach) and then \textit{2-opt} is used for the intensification phase. It is important to notice that $delta$ is
how much the solution cost would improve (decrease by).

\begin{algorithm}[ht]
\caption{Tabu Search}
\label{alg:tabu}
\begin{algorithmic}
  \Procedure{tabusearch}{$solution$}
    \State\Call{greedy}{solution}
    \State $tabulist \gets \emptyset$

    \While{$!stop$}
        \State $move \gets \Call{findbestswapnotabu}{solution, tabulist}$
        \State $delta \gets \Call{delta}{move}$
        \State $solution \gets \Call{apply}{solution, move}$
        \If{$delta \leq 0$}
          \State $tabulist \gets tabulist \cup \{move\}$
        \EndIf
        \State $\Call{removeold}{tabulist}$
    \EndWhile

  \EndProcedure

\end{algorithmic}
\end{algorithm}

The procedure begins by solving the problem with a greedy approach and the tabu list
is empty. All the procedure is executed in a while loop with a generic condition. We could
decide to exit the loop in a large variety of ways. In our experiments we leave the loop
after a predefined time-limit. Other possibilities can involve a maximum number of iterations, a lack of any improvement in the last iterations or any other kind of stopping criteria.

At every iteration of the loop, we find the best move that is not in the tabu list. If this move has a positive delta (meaning that its application would lead to an
improvement in the solution) we apply it and jump to the next iteration. If the best
move has a negative delta we apply it anyway, adding it to the list of the tabu moves.

It is then important to define a way to remove moves from the list. In the pseudo-code we define it with a generic ``removeold'' function, in practice there are many ways the remove tabu entries.

We define ``tenure'' of the tabu method the window of iteration in which tabu moves are considered. For example a tenure of $100$ iterations would mean that we remove
from the tabu list all moves that weren't added in the last $100$ iterations.

This \textit{hyperparameter} is crucial for the effectiveness of the method. In our implementation we experimented with a fixed tenure (dependent on the number of
nodes of the instance) and with some functions of the number of the current iteration. Some good results were obtained with a sinusoidal function.

This approach using variable values for the tenure can be quite effective compared to a fixed one as we will se in the chapted dedicated to experimental results.
An intuition behind this can be given by the fact that varying the tenure can be beneficial to the search of a new minimum because it tries to solve both the problems of having a tenure that is too big and one that is too small.
A small tenure can be problematic because we cannot escape the local minimum far enough. A big tenure has the disadvantage that we might find ourselves in a situation where the move needed to reach a new local minimum is blocked by the
tabu list. Having a variable tenure can sometimes, on the long run, avoid the drawbacks of a fixed approach.

\subsection{VNS (Variable Neighborhood Search)}
In \textit{Tabu Search} we have seen two different phases:
\begin{itemize}
        \item the \textit{Intensification Phase} to move towards a better solution
        \item the \textit{Diversification Phase} in which we try to `escape` from a local minimum using only
        non-tabu moves
\end{itemize}
With this approach, we may spend a considerable amount of time without achieving any improvement in
solution quality (Diversification Phase). One potential solution could involve restarting from a random
point each time we encounter a local minimum (Multistart). However, this approach would significantly
prolong the Intensification Phase. Moreover, when encountering a local minimum, it is probable
that the solution obtained is not entirely incorrect but requires adjustment in some aspect.
VNS is a metaheuristic that employs minimal permutations of the solution to evade local minima, as
opposed to restarting from a completely different starting point.
With this approach, we transition from one solution to a better one using the 2-opt method until reaching
a local minimum. Subsequently, we perform a random number of permutations involving three nodes (3-opt)
to navigate away from the minimum. This approach also ensures that we never revert to a previously visited
solution in the last 2-opt application, as it cannot be achieved through a sequence of permutations of
three nodes.
This is a simplified version of the algorithm; state-of-the-art techniques also allow for larger permutations
of nodes (e.g., 4-opt, 5-opt, etc.) instead of multiple 3-opt operations.
Algorithm~\ref{alg:vns} shows a very abstract description of the procedure

\begin{algorithm}[ht]
\caption{VNS}
\label{alg:vns}
\begin{algorithmic}
\Procedure{vns}{$solution$}
        \State\Call{greedy}{solution}
        \While{$!stop$}
                \State $move \gets \Call{findbest2optswap}{solution}$
                \State $delta \gets \Call{delta}{move}$
                \If{$delta \leq 0$}
                        \For{$\Call{random}{range}$}
                                \State $move \gets \Call{find3optswap}{solution}$
                                \State $solution \gets \Call{apply}{solution, move}$
                        \EndFor
                \Else
                        \State $solution \gets \Call{apply}{solution, move}$
                \EndIf
        \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\clearpage

\section{Exact Methods}
Although, as we will see, a good heuristic can achieve very good results, it is also
important to consider approaches that lead to an optimal solution of the
TSP problem.

All of the techniques that we will cover are based on integer linear programming. It is
hence useful to provide an integer linear formulation for the problem.
Many formulations exist, but the one we will focus on is the one proposed by
Dantzig, Fulkerson and Johnson. % TODO cite https://pubsonline.informs.org/doi/10.1287/opre.2.4.393

Initally we define an undirected complete graph $G=(V, E)$.
We will the use $c_e : E \rightarrow \mathbb{R}^+$ to indicate the cost of
an edge.
One of the ways to compute this cost function is to place all the points
on a 2d surface and compute their eucledian distance.

The variables of this model are
\begin{equation*}
        x_e =
        \left\{
                \begin{array}{l}
                1 \text{ the path uses edge $e$}\\
                0 \text{ otherwise }
        \end{array}
        \right.
\end{equation*}

The model, that makes use of the subtour elimination constraints, is the
following
\begin{equation}
  \begin{aligned}
\min & \sum_{e \in E} c_e x_e \\
\text{s.t.}
& \sum_{e \in \delta(h)} x_{e} = 2 \quad \forall h \in V \\
& \sum_{e \in E(S)} x_{e} \leq |S| - 1 \quad \forall S \subset V : |S| \geq 3 \\
& x_{e} \in \{0, 1\} \quad \forall e \in E.
\end{aligned}
\end{equation}
$\delta(h)$, where $h$ is a node, is defined as the set of edges that are
incident to $h$. $E(S)$, where $S$ is a set of nodes, is the est of all the
the edges with both extremities in $S$.

It is important to know that the number of SECs is exponential in the number of
the nodes. Hence, it is infeasible to generate all of them from the beginning
with the size of the instances we are interested in.

The techniques that will be shown in this section are different ways to generate
the subtour elimination constraints. We will use CPLEX to solve the MIP
problems.
% TODO more on cplex
\subsection{Benders' loops}
\label{ssec:benders}
The first technique we will explore is the so-called ``Benders' loops'' technique.
The basic idea behind this approach is the iterated use of a MIP solver (like CPLEX)
as a black-box. More precisely, we will start by providing the solver with
a model lacking all the separation constraints. The solution of this model,
with all probability, is going to contain more than one loop. We will then
generate some constraints "cutting" the solution we have found.
The model, updated with the new constraints, is then passed to the solver
and solved again. This loop will be repeated until a solution composed by
only one component is found.
The process is described in Figure~\ref{fig:benders}.

\begin{figure}[ht]
        \caption{Solving using Benders' loops}
        \label{fig:benders}
        \centering
        \includegraphics[width=340pt]{assets/bendersloops.drawio.pdf}
\end{figure}

It is very important to note that the ``optimal solution'' output of the
MIP solver is not the optimal solution of the TSP problem (exception made
for the last iteration of the loop). That solution is optimal only for the
current set of subtour elimination constraints.

As we will see in Section~\ref{sec:results}, although this method allows us to
solve some instances it has a major flow. By using the MIP solver as a black
box, we have to restart all the process every time a new solution (that
contains subtours) is found. This creates on big problem: the branch-and-bound
tree will be discarded and the search for the optimal solution will start again.
Even though throwing away the search tree can be beneficial sometimes, % TODO add dyamic search citation
it needs to be done following some precise criteria and at the correct time.

\subsubsection{Patching}
At the end of the Bender's loop we are guaranteed to obtain a solution to the TSP problem.
However, if we exit the loop because of the time limit, we'll return a solution containing more than
one loop. To solve this problem we implemented the so-called `Patching Heuristic'.
The idea behind this algorithm is to find a way to merge together all the components returned by the MIP solver
so that, at each iteration of the Bender's loop, we have a solution composed of only one component.
In this way, if we exit the Bender's loop because of the time limit, we'll always be able to return a
correct solution to the problem.
Figure \ref{fig:patching} shows an example of patching.

\begin{figure}[H]
        \caption{Example of patching}
        \label{fig:patching}
        \centering
        \begin{subfigure}{.5\textwidth}
                \centering
                \def\svgwidth{.7\linewidth}
                \import{assets}{pre_patching.pdf_tex}
                \caption{MIP solver solution}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
                \centering
                \def\svgwidth{.7\linewidth}
                \import{assets}{post_patching.pdf_tex}
                \caption{Patched solution}
        \end{subfigure}
\end{figure}

The Patching Heuristic aims to find the optimal way to merge two different components. As shown 
in Figure \ref{fig:patching}, the patched solution is the one that minimizes the cost of the
resulting single component.
An abstract description of the techinque is presented in Algorithm \ref{alg:patching}.

\begin{algorithm}[ht]
\caption{Bender's loop + Patching Heuristic}
\label{alg:patching}
\begin{algorithmic}
\Procedure{benders+patching}{$model$}
        \While{$!stop$}
                \State $solution \gets \Call{mipsolver}{model}$
                \If{$\Call{components}{solution} > 1$}
                        \State $model \gets model \cup SEC$
                        \State $solution \gets \Call{patchcomponents}{solution}$
                \Else
                        \State $break$
                \EndIf
        \EndWhile
        \State \Return $solution$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Callback}
The technique that will be presented in this section solves the problem
described in Section~\ref{ssec:benders} by generating the constraints when
the MIP solver finds a new canditate solution. In order to achieve this,
CPLEX provides us a C interface that we can use to control the execution
of the branch-and-cut.

The process will go as follows: first, we will provide the solver with the
ILP model, without the cuts. Whenever the solver finds a candidate solution,
the execution will pass to a function that we have defined. This function
will count the number of tours present in the solution. If this number is $1$,
hence the solution is feasbile, we return from the function and pass the
execution back to the solver. If the ``tour'' is composed by many subtours,
we will generate the approriate cuts before returning the exectution to
cplex.

It is possible to see a visual description of the process in
Figure~\ref{fig:callback}.

\begin{figure}[ht]
        \caption{Solving using candidate callback}
        \label{fig:callback}
        \centering
        \includegraphics[width=340pt]{assets/callback.drawio.pdf}
\end{figure}

As we can see in the the diagram, we will need a way to generate valid cuts
whenever the MIP solver has a new canditate solution. To do this we ``install''
a callback. CPLEX allows us to pass a function to it, that will be called when
a specific condition is met. In our case we will use one that is called when CPLEX
has found a new candidate for an integer-feasible solution.
% TODO maybe be more specific about the name of the condition

Inside this function we will count the number of components. If the number of
components is greater then one, we will generate the corresponding subtour
elimination constraints and reject the solution.

\newpage

\section{Matheuristics}
In this section we will explore another useful set of techniques that we will call
\textit{matheuristics}.
The core idea behind these techniques is to use heuristics in conjunction with the
mathematical models that one would normally use to solve the problem optimally.
One of the advantages of such approach is that they can be applied to different problems,
with little to none needed adaptations.

What is important to note here is that matheuristics isn't a new class of techniques
for solving optimization problems but it is instead a nomenclature that was given
to the concept of using mathematical tools to design heuristics~\cite{boschetti2022matheuristics}.

The term was coined to give a name to the first edition of a workshop in Bertinoro, Italy
in 2006.

In this report we will focus on two matheuristic techniques.
The first one that will be explored is Diving, while the second one is Local Branching.

As we will see in Section~\ref{sec:results}, these techniques will be used to solve
heuristically instances bigger than what we will be able to solve optimally using approaches
based on the branch-and-cut.

\subsection{Diving}
The first technique that we are going to explore is \textit{Diving}.

\textit{Diving} is a refinement algorithm, meaning that it is a way to improve a feasible
sub-optimal solution.

The idea goes as follows: we start from a solution (that can be obtained with an heuristic or in
the worst case generated randomly) and we fix part of the selected edges. The fixing of the
edges will be done by adding constraints to the model, allowing us to use the MIP solver
as a black box.

We can fix all the edges
\begin{equation*}
  x_{e}=1 \quad \forall e \in \tilde{E}
\end{equation*}
where
\begin{equation*}
  \tilde{E} \subset E^{H} \coloneq \left\{ e \in E : x_{e}^{H} = 1 \right\}
\end{equation*}
meaning that the edges we fix are a subset of the edges that are part of the heuristic solution.

What is left now is deciding how many and which edges to fix. The approach that will be explored
in this report is choosing a number of edges (which will become an hyperparameter of our system)
and select the edges to fix randomly.

\subsection{Local Branching}
The second approach this report will focus on is \textit{Local Branching}.

\textit{Local Branching} was initially introduced in a paper by Fischetti and Lodi~\cite{fischetti2003local} and was then applied by many other researchers over the following
years.

The idea behind this technique is to decide the number of edges to fix, like we did for Diving,
but then leaving the choice of \textit{what edges} to the MIP solver.

This is possible thanks to the Local Branching constraints, which, in their simplified
asymmetric version are constructed like this

\begin{equation*}
  \sum_{e : x_{e}^{H}=1} x_{e} \geq n - k
\end{equation*}

This forces at least $n-k$ edges of the heuristic solution we apply this method on to be
set to $1$ in the solution of the model this constraint is part of.

We say that this is an \textit{asymmetric} version because the ``original'' Local Branching
constraint would consider also the variables set to $0$ becoming $1$.
In the TSP problem it would make no sense to add such constraints since every feasible
solution has the same number of variables set to $1$.

The \textit{simplified} comes instead from the fact that we know a priori the number of
variables set to $1$ in a solution. This allows us to put $n$ directly into the right-hand-side
of the constraint and avoid building a more complicated cut where we consider the difference
of variables set to $1$ between the input heuristic solution and the solution of the model.

In its original version the Local Branching constraint introduced by Fischetti, that
could be written as
\begin{equation*}
\sum_{i \in I: \hat{x}_i = 0} x_i + \sum_{i \in I: \hat{x}_i = 1} (1 - x_i) \leq k \text{,}
\end{equation*}
limits the Hamming distance between the heuristic and the solution we will find to
a value $k$.

This can be seen as a technique to use the MIP solver as a black box to find
the best $k$-opt moves in a feasible solution.

This leaves us with one hyperparameter to tune: the value of $k$ for each iteration.
Our approach in this report will be to decide an initial value of $k$ that we will call
$k_{0}$. The first iteration of the local branching will use $k_{1} = k_{0}$.
We will keep running the solver with the same value of $k$ until no improved
solution can be found. At this point, say iteration $t$, we increment $k$ by a delta:
$k_{t+1} = k_{t} + \Delta k$. This process will continue until we reach the timelimit.

\newpage

\section{Results}
\label{sec:results}
This section is structured as follows: first we will try to optimize the hyperparameters
of the approaches that have some. Then, we will compare the methods in their best configuration,
searching for the best one.

Naturally, the main difference we have among the approaches that we present is if we are computing
an optimal solution or if we are applying an heuristic. In the former case, the metric we are going
to compare our systems on is the time it took them to arrive to the optimal solution.
For the latter we instead fix a time limit (the maximum time we allow our system to run for) and
we seek the system that found the best solution (in our case the smallest one, since the TSP
is a minimization problem).

Before going through the experiments that were performed, it can be useful to discuss some
parameters that will be used in this section.
The instances will be generated randomly and we will refer to them using the seed we
initialize our number generator with. For reproducibility reasons, they were exported
and are available here: %TODO insert link with the instances.
The size of the instances will be
\begin{itemize}
  \item $300$ for the optimal methods
  \item $1000$ for matheuristics
  \item $1500$ for heuristics
\end{itemize}

For what concernes the value of the time limit, we will use $120$ seconds for all the
experiments.

To make comparisons a little bit more statistically valid all the systems will be run on
$20$ different instances of the same size. We will then take the results for each instance
and plot the Performance Profile. All our considerations will be based on these plots.


\newpage

\bibliographystyle{plain}
\bibliography{thesis}

\end{document}
